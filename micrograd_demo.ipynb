{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e17b1d8",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Ensure your `Value`, `Neuron`, `Layer`, and `MLP` classes are correctly implemented and saved in a file named: `micrograd_engine.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ee0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single-file script demonstrating a micrograd-style MLP training \n",
    "# on both the non-linear 'make_moons' and the more separable 'load_iris' \n",
    "# datasets, using mini-batching for speed.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from sklearn.datasets import make_moons, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from micrograd_engine import MLP, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, y, batch_size):\n",
    "    \"\"\"Generates mini-batches from the data.\"\"\"\n",
    "    num_samples = len(X)\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        batch_indices = indices[start:end]\n",
    "        yield X[batch_indices], y[batch_indices]\n",
    "\n",
    "\n",
    "def train_and_visualize(dataset_name, X_full, y_full, n_layers, n_neurons_per_layer, \n",
    "                        epochs=1000, lr=0.01, alpha=1e-4, batch_size=32):\n",
    "    \"\"\"\n",
    "    Sets up, trains, and visualizes an MLP using mini-batch gradient descent.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"STARTING TRAINING FOR: {dataset_name.upper()} DATASET (Batch Size: {batch_size})\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Preprocessing\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X_full)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Model Initialization\n",
    "    n_features = X_train.shape[1]\n",
    "    n_output = 1\n",
    "    layer_sizes = [n_neurons_per_layer] * n_layers + [n_output]\n",
    "    model = MLP(n_features, layer_sizes)\n",
    "\n",
    "    print(f\"Architecture: {n_features} -> {' -> '.join(map(str, layer_sizes))}\")\n",
    "    print(f\"Total parameters: {len(model.parameters())}\")\n",
    "    \n",
    "    history = []\n",
    "\n",
    "    # --- Training Loop (Mini-Batch Gradient Descent) ---\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Iterate over batches\n",
    "        for X_batch, y_batch in get_batches(X_train, y_train, batch_size):\n",
    "            \n",
    "            # 1. Forward Pass (Graph built for current batch ONLY)\n",
    "            X_batch_value = [list(map(Value, x_in)) for x_in in X_batch]\n",
    "            ypred = [model(x) for x in X_batch_value] \n",
    "            \n",
    "            # --- Loss Calculation ---\n",
    "            \n",
    "            loss_sum = Value(0.0)\n",
    "            for yt, yout in zip(y_batch, ypred):\n",
    "                # Ensure predicted_value is a single Value object \n",
    "                predicted_value = yout[0] if isinstance(yout, list) else yout \n",
    "                loss_sum += (predicted_value - Value(yt))**2\n",
    "            data_loss = loss_sum\n",
    "            \n",
    "            # L2 Regularization Loss\n",
    "            reg_loss = alpha * sum((p * p) for p in model.parameters())\n",
    "            total_loss = data_loss + reg_loss\n",
    "            \n",
    "            # --- Backward Pass & Update ---\n",
    "            \n",
    "            for p in model.parameters(): p.grad = 0.0\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # Update parameters\n",
    "            for p in model.parameters():\n",
    "                p.data += -lr * p.grad\n",
    "                \n",
    "            epoch_loss += total_loss.data\n",
    "            \n",
    "        history.append(epoch_loss)\n",
    "\n",
    "        if (epoch + 1) % 100 == 0 or epoch == epochs - 1:\n",
    "            # --- Reporting / Evaluation ---\n",
    "            X_epoch_value = [list(map(Value, x_in)) for x_in in X_train]\n",
    "            y_pred_final = [model(x) for x in X_epoch_value] \n",
    "            \n",
    "            loss_sum_final = Value(0.0)\n",
    "            for yt, yout in zip(y_train, y_pred_final):\n",
    "                predicted_value = yout[0] if isinstance(yout, list) else yout \n",
    "                loss_sum_final += (predicted_value - Value(yt))**2\n",
    "                \n",
    "            final_data_loss = loss_sum_final\n",
    "            final_reg_loss = alpha * sum((p * p) for p in model.parameters())\n",
    "            final_total_loss = final_data_loss + final_reg_loss\n",
    "            \n",
    "            y_out_data = np.array([(p[0] if isinstance(p, list) else p).data for p in y_pred_final])\n",
    "            predictions = np.sign(y_out_data)\n",
    "            accuracy = np.mean(predictions == y_train) * 100\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1:4d} | Total Loss: {final_total_loss.data:.4f} | Train Acc: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "    # --- Visualization Functions ---\n",
    "\n",
    "    def calculate_predictions(X_data, model):\n",
    "        X_value = [list(map(Value, x_in)) for x_in in X_data]\n",
    "        predictions = [model(x) for x in X_value]\n",
    "        return np.array([(p[0] if isinstance(p, list) else p).data for p in predictions])\n",
    "\n",
    "    # Plot Loss History\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history)\n",
    "    plt.title(f\"{dataset_name.upper()} Loss History (Final Loss: {final_total_loss.data:.4f})\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Epoch Total Loss (Sum of Batches)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Decision Boundary \n",
    "    if X.shape[1] == 2:\n",
    "        # Plotting for 2D data (Make Moons)\n",
    "        h = 0.02\n",
    "        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "        grid_input = np.c_[xx.ravel(), yy.ravel()]\n",
    "        Z = calculate_predictions(grid_input, model)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.8)\n",
    "        plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='coolwarm', edgecolors='k')\n",
    "        plt.title(f\"{dataset_name.upper()} Decision Boundary (Batch Size: {batch_size})\")\n",
    "        plt.xlabel(\"Feature 1 (Scaled)\")\n",
    "        plt.ylabel(\"Feature 2 (Scaled)\")\n",
    "        plt.colorbar(label='Model Output (tanh)')\n",
    "        plt.show()\n",
    "\n",
    "    elif X.shape[1] == 4:\n",
    "        # Plotting for 4D data (Iris) using the first 2 features for visualization\n",
    "        h = 0.02\n",
    "        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "        # Fill non-visualized features with their training set mean\n",
    "        grid_2d = np.c_[xx.ravel(), yy.ravel()]\n",
    "        mean_features = X_train[:, 2:].mean(axis=0)\n",
    "        grid_full = np.c_[grid_2d, np.tile(mean_features, (grid_2d.shape[0], 1))]\n",
    "\n",
    "        Z = calculate_predictions(grid_full, model)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        feature_names = load_iris().feature_names\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.8)\n",
    "        plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='coolwarm', edgecolors='k')\n",
    "        plt.title(f\"{dataset_name.upper()} Decision Boundary (First 2 Features, Batch Size: {batch_size})\")\n",
    "        plt.xlabel(f\"Feature 1: {feature_names[0]} (Scaled)\")\n",
    "        plt.ylabel(f\"Feature 2: {feature_names[1]} (Scaled)\")\n",
    "        plt.colorbar(label='Model Output (tanh)')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Make Moons (Non-Linear Classification)\n",
    "X_moons, y_moons_int = make_moons(n_samples=200, noise=0.15, random_state=42)\n",
    "y_moons = np.where(y_moons_int == 0, -1.0, 1.0).astype(float) \n",
    "\n",
    "train_and_visualize(\"make_moons\", X_moons, y_moons, n_layers=2, n_neurons_per_layer=16, \n",
    "                        epochs=200, lr=0.05, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Iris (Linearly Separable Classification)\n",
    "iris = load_iris()\n",
    "X_iris_full = iris.data\n",
    "y_iris = np.where(iris.target == 0, 1.0, -1.0).astype(float)\n",
    "train_and_visualize(\"iris\", X_iris_full, y_iris, n_layers=1, n_neurons_per_layer=8, \n",
    "                        epochs=100, lr=0.01, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266127b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
